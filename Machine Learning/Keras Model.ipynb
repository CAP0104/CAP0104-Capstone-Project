{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from dataset import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TRAIN_PATH = 'tf_train.csv'\n",
    "VALIDATION_PATH = 'tf_validation.csv'\n",
    "#TEST_PATH = 'test_processed.csv'\n",
    "MODEL_DIR = 'kmodel2'\n",
    "SUBMISSION_NAME = 'submission_keras.csv'\n",
    "TEST_PATH = 'test_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "STEPS = 100000\n",
    "BATCH_SIZE = 512\n",
    "DATASET_SIZE = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "               'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [['nokey'], [1.0], ['2009-06-15 17:26:21 UTC'], [-74.0], [40.0], [-74.0], [40.7], [1.0], [2009], [6], [15],\n",
    "            [17], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['pickup_longitude_binned'] = pd.qcut(df['pickup_longitude'], 16, labels=False)\n",
    "    df['dropoff_longitude_binned'] = pd.qcut(df['dropoff_longitude'], 16, labels=False)\n",
    "    df['pickup_latitude_binned'] = pd.qcut(df['pickup_latitude'], 16, labels=False)\n",
    "    df['dropoff_latitude_binned'] = pd.qcut(df['dropoff_latitude'], 16, labels=False)\n",
    "\n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(df):\n",
    "    #df = df.drop('pickup_datetime', axis=1)\n",
    "    \n",
    "    df['pickup_longitude_binned'] = pd.qcut(df['pickup_longitude'], 16, labels=False)\n",
    "    df['dropoff_longitude_binned'] = pd.qcut(df['dropoff_longitude'], 16, labels=False)\n",
    "    df['pickup_latitude_binned'] = pd.qcut(df['pickup_latitude'], 16, labels=False)\n",
    "    df['dropoff_latitude_binned'] = pd.qcut(df['dropoff_latitude'], 16, labels=False)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relevant_distances(df):\n",
    "    # Add airpot distances and downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    df['downtown_pickup_distance'] = manhattan(ny[1], ny[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['downtown_dropoff_distance'] = manhattan(ny[1], ny[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['jfk_pickup_distance'] = manhattan(jfk[1], jfk[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['jfk_dropoff_distance'] = manhattan(jfk[1], jfk[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['ewr_pickup_distance'] = manhattan(ewr[1], ewr[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['ewr_dropoff_distance'] = manhattan(ewr[1], ewr[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['lgr_pickup_distance'] = manhattan(lgr[1], lgr[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['lgr_dropoff_distance'] = manhattan(lgr[1], lgr[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered(df):\n",
    "    lat1 = df['pickup_latitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    euclidean = (latdiff ** 2 + londiff ** 2) ** 0.5\n",
    "\n",
    "    # Add new features\n",
    "    df['latdiff'] = latdiff\n",
    "    df['londiff'] = londiff\n",
    "    df['euclidean'] = euclidean\n",
    "    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    # One-hot encoding columns\n",
    "    # Note, this is note the best way to one-hot encode features, but probably the simplest and will work here\n",
    "    df = pd.get_dummies(df, columns=['weekday'])\n",
    "    df = pd.get_dummies(df, columns=['month'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_function(features, labels=None, shuffle=False):\n",
    "    input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"raw_input\": features},\n",
    "        y=labels,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset2(filename, mode, features_cols, label_col, default_value, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.io.decode_csv(records=value_column, record_defaults=default_value)\n",
    "            features = dict(zip(features_cols, columns))\n",
    "            label = features.pop(label_col)\n",
    "\n",
    "            features = tf.cast(features, dtype=tf.float32)\n",
    "            features = {\"raw_input\": add_engineered(features)}\n",
    "            label = tf.cast(label, dtype=tf.float32)\n",
    "            return features, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read lines from text files\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "        textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "        dataset = textlines_dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        batch_features, batch_labels = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "\n",
    "        return batch_features, batch_labels\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load values in a more compact form\n",
    "data_train_types = {'key': 'str',\n",
    "             'fare_amount': 'float32',\n",
    "             'pickup_datetime': 'str',\n",
    "             'pickup_longitude': 'float32',\n",
    "             'pickup_latitude': 'float32',\n",
    "             'dropoff_longitude': 'float32',\n",
    "             'dropoff_latitude': 'float32',\n",
    "             'passenger_count': 'uint8',\n",
    "             'year': 'uint16',\n",
    "             'month': 'uint8',\n",
    "             'day': 'uint8',\n",
    "             'hour': 'uint8',\n",
    "             'weekday': 'uint8',\n",
    "             'night': 'uint8',\n",
    "             'late_night': 'uint8'}\n",
    "\n",
    "data_train_names = ['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "              'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']\n",
    "\n",
    "data_test_types = {'key': 'str',\n",
    "             'pickup_datetime': 'str',\n",
    "             'pickup_longitude': 'float32',\n",
    "             'pickup_latitude': 'float32',\n",
    "             'dropoff_longitude': 'float32',\n",
    "             'dropoff_latitude': 'float32',\n",
    "             'passenger_count': 'uint8',\n",
    "             'year': 'uint16',\n",
    "             'month': 'uint8',\n",
    "             'day': 'uint8',\n",
    "             'hour': 'uint8',\n",
    "             'weekday': 'uint8',\n",
    "             'night': 'uint8',\n",
    "             'late_night': 'uint8'}\n",
    "\n",
    "data_test_names = ['key', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "              'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE, dtype=data_train_types, usecols=[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], names=data_train_names)\n",
    "test = pd.read_csv(TEST_PATH, dtype=data_test_types, usecols=[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13], names=data_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "train = process(train)\n",
    "test = process(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "train = add_relevant_distances(train)\n",
    "test = add_relevant_distances(test)\n",
    "\n",
    "train = add_engineered(train)\n",
    "test = add_engineered(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "dropped_columns = ['pickup_longitude', 'pickup_latitude',\n",
    "                   'dropoff_longitude', 'dropoff_latitude']\n",
    "train_clean = train.drop(dropped_columns, axis=1)\n",
    "test_clean = test.drop(dropped_columns + ['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df).astype(np.float32)\n",
    "validation_df_scaled = scaler.transform(validation_df).astype(np.float32)\n",
    "test_scaled = scaler.transform(test_clean).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(512, activation='relu', input_shape=(train_df_scaled.shape[1],), name='raw'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, name='predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "raw (Dense)                  (None, 512)               20992     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 200,129\n",
      "Trainable params: 198,113\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.estimator.RunConfig(model_dir=MODEL_DIR, save_summary_steps=5000, save_checkpoints_steps=5000)\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_function(train_df_scaled, train_labels, True),\n",
    "                                    max_steps=STEPS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_function(validation_df_scaled, validation_labels, True),\n",
    "                                  steps=1000, throttle_secs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'kmodel2', '_tf_random_seed': None, '_save_summary_steps': 5000, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 5000 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='kmodel2\\\\keras\\\\keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: kmodel2\\keras\\keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 26 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into kmodel2\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 150.78896, step = 0\n",
      "INFO:tensorflow:global_step/sec: 68.1127\n",
      "INFO:tensorflow:loss = 217.30273, step = 100 (1.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.117\n",
      "INFO:tensorflow:loss = 172.50548, step = 200 (0.832 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.543\n",
      "INFO:tensorflow:loss = 221.71658, step = 300 (0.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.026\n",
      "INFO:tensorflow:loss = 173.04135, step = 400 (0.855 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.472\n",
      "INFO:tensorflow:loss = 204.95282, step = 500 (0.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.627\n",
      "INFO:tensorflow:loss = 192.76877, step = 600 (0.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.307\n",
      "INFO:tensorflow:loss = 130.93587, step = 700 (0.831 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.696\n",
      "INFO:tensorflow:loss = 145.19107, step = 800 (0.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.656\n",
      "INFO:tensorflow:loss = 137.22766, step = 900 (0.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.886\n",
      "INFO:tensorflow:loss = 129.7089, step = 1000 (0.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.972\n",
      "INFO:tensorflow:loss = 112.71602, step = 1100 (0.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 114.658\n",
      "INFO:tensorflow:loss = 403.4195, step = 1200 (0.873 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.043\n",
      "INFO:tensorflow:loss = 131.68333, step = 1300 (0.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.812\n",
      "INFO:tensorflow:loss = 116.52459, step = 1400 (0.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.302\n",
      "INFO:tensorflow:loss = 191.38408, step = 1500 (0.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.258\n",
      "INFO:tensorflow:loss = 162.20784, step = 1600 (0.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.116\n",
      "INFO:tensorflow:loss = 161.61862, step = 1700 (0.839 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.581\n",
      "INFO:tensorflow:loss = 98.77615, step = 1800 (0.837 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.551\n",
      "INFO:tensorflow:loss = 103.092255, step = 1900 (0.844 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.313\n",
      "INFO:tensorflow:loss = 105.71482, step = 2000 (0.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.269\n",
      "INFO:tensorflow:loss = 134.5454, step = 2100 (0.830 sec)\n",
      "INFO:tensorflow:global_step/sec: 115.882\n",
      "INFO:tensorflow:loss = 122.825905, step = 2200 (0.864 sec)\n",
      "INFO:tensorflow:global_step/sec: 119.829\n",
      "INFO:tensorflow:loss = 65.76456, step = 2300 (0.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.346\n",
      "INFO:tensorflow:loss = 88.870636, step = 2400 (0.860 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.35\n",
      "INFO:tensorflow:loss = 144.31482, step = 2500 (0.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.842\n",
      "INFO:tensorflow:loss = 120.275734, step = 2600 (0.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.533\n",
      "INFO:tensorflow:loss = 108.719696, step = 2700 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 120.013\n",
      "INFO:tensorflow:loss = 84.88016, step = 2800 (0.834 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.753\n",
      "INFO:tensorflow:loss = 61.9506, step = 2900 (0.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 107.704\n",
      "INFO:tensorflow:loss = 70.03134, step = 3000 (0.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 112.099\n",
      "INFO:tensorflow:loss = 64.68435, step = 3100 (0.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.629\n",
      "INFO:tensorflow:loss = 58.913143, step = 3200 (0.850 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3280...\n",
      "INFO:tensorflow:Saving checkpoints for 3280 into kmodel2\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3280...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-06-01T13:33:13Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from kmodel2\\model.ckpt-3280\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Inference Time : 2.15287s\n",
      "INFO:tensorflow:Finished evaluation at 2021-06-01-13:33:15\n",
      "INFO:tensorflow:Saving dict for global step 3280: global_step = 3280, loss = 77.71275, mae = 7.5553284\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3280: kmodel2\\model.ckpt-3280\n",
      "INFO:tensorflow:Loss for final step: 86.700714.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': 77.71275, 'mae': 7.5553284, 'global_step': 3280}, [])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = keras.estimator.model_to_estimator(keras_model=model, config=run_config)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from kmodel2\\model.ckpt-3280\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Output complete\n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_NAME = 'sample_submission.csv'\n",
    "prediction = estimator.predict(input_function(test_scaled))\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction)\n",
    "output_submission(test, prediction_df, 'key', 'fare_amount', SUBMISSION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_scaled.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for feature_name in CSV_COLUMNS:\n",
    "    vocabulary = train[feature_name]\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_estimator_Fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "  tf.feature_column.make_parse_example_spec([feature_columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.export_saved_model('keras_estimator_savedmodel', input_estimator_Fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n",
    "    features = tf.compat.v1.placeholder(dtype=tf.float32,\n",
    "                                         shape=[None],\n",
    "                                         name='input_tensor')\n",
    "    #receiver_tensors = {'input_csv': serialized_tf_example}\n",
    "    #feature = tf.io.parse_example(serialized_tf_example, features)\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        features=features, receiver_tensors=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'pickup_datetime' : tf.compat.v1.placeholder(tf.string, [None]),\n",
    "        'pickup_longitude' : tf.compat.v1.placeholder(tf.float32, [None]),\n",
    "        'pickup_latitude' : tf.compat.v1.placeholder(tf.float32, [None]),\n",
    "        'dropoff_longitude' : tf.compat.v1.placeholder(tf.float32, [None]),\n",
    "        'dropoff_latititude' : tf.compat.v1.placeholder(tf.float32, [None]),\n",
    "        'passenger_count': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'year': tf.compat.v1.placeholder(tf.uint16, [None]),\n",
    "        'month': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'day': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'hour': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'weekday': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'night': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        'late_night': tf.compat.v1.placeholder(tf.uint8, [None]),\n",
    "        #'raw_input' : tf.compat.v1.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, \n",
    "                                                    feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        'raw_input' : tf.compat.v1.placeholder(tf.float32, [1, 40, None])\n",
    "    }\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    return tf.estimator.export.ServingInputReceiver(features, \n",
    "                                                    feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from kmodel2\\model.ckpt-3280\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: keras_estimator_savedmodel\\temp-1622537169\\saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'keras_estimator_savedmodel\\\\1622537169'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.estimator.Estimator.export_saved_model('keras_estimator_savedmodel', serving_input_receiver_fn())\n",
    "estimator.export_saved_model('keras_estimator_savedmodel', serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing weight file ./keras_estimator_savedmodel/json_model\\model.json..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-01 15:46:24.294182: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-06-01 15:46:24.295264: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-06-01 15:46:31.616167: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-06-01 15:46:31.619347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\n",
      "2021-06-01 15:46:32.343735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: GeForce 920M computeCapability: 3.5\n",
      "coreClock: 0.954GHz coreCount: 2 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 13.41GiB/s\n",
      "2021-06-01 15:46:32.348488: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-06-01 15:46:32.352144: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2021-06-01 15:46:32.356205: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2021-06-01 15:46:32.360423: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2021-06-01 15:46:32.364776: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2021-06-01 15:46:32.368178: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\n",
      "2021-06-01 15:46:32.371745: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2021-06-01 15:46:32.375608: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2021-06-01 15:46:32.376549: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-06-01 15:46:32.379417: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-01 15:46:32.381707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-01 15:46:32.382345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n",
      "2021-06-01 15:46:32.382984: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-06-01 15:46:33.235606: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n",
      "2021-06-01 15:46:34.166892: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2021-06-01 15:46:34.167943: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-06-01 15:46:34.175623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: GeForce 920M computeCapability: 3.5\n",
      "coreClock: 0.954GHz coreCount: 2 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 13.41GiB/s\n",
      "2021-06-01 15:46:34.180497: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-06-01 15:46:34.184788: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2021-06-01 15:46:34.188852: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2021-06-01 15:46:34.192660: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2021-06-01 15:46:34.196690: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2021-06-01 15:46:34.201081: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\n",
      "2021-06-01 15:46:34.205393: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2021-06-01 15:46:34.209796: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2021-06-01 15:46:34.210824: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-06-01 15:46:34.326769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-01 15:46:34.327491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-06-01 15:46:34.327840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-06-01 15:46:34.328450: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-06-01 15:46:34.456362: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
      "\n",
      "2021-06-01 15:46:36.625560: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.018ms.\n",
      "  model_pruner: Graph size after: 207 nodes (-38), 241 edges (-38), time = 6.468ms.\n",
      "  constant_folding: Graph size after: 207 nodes (0), 241 edges (0), time = 53.613ms.\n",
      "  arithmetic_optimizer: Graph size after: 207 nodes (0), 241 edges (0), time = 20.992ms.\n",
      "  dependency_optimizer: Graph size after: 186 nodes (-21), 206 edges (-35), time = 4.616ms.\n",
      "  model_pruner: Graph size after: 186 nodes (0), 206 edges (0), time = 2.724ms.\n",
      "  constant_folding: Graph size after: 186 nodes (0), 206 edges (0), time = 10.039ms.\n",
      "  arithmetic_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 7.058ms.\n",
      "  dependency_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 2.574ms.\n",
      "  debug_stripper: debug_stripper did nothing. time = 0.245ms.\n",
      "  model_pruner: Graph size after: 186 nodes (0), 206 edges (0), time = 1.32ms.\n",
      "  constant_folding: Graph size after: 186 nodes (0), 206 edges (0), time = 9.453ms.\n",
      "  arithmetic_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 8.524ms.\n",
      "  dependency_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 4.334ms.\n",
      "  model_pruner: Graph size after: 186 nodes (0), 206 edges (0), time = 2.678ms.\n",
      "  constant_folding: Graph size after: 186 nodes (0), 206 edges (0), time = 7.646ms.\n",
      "  arithmetic_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 7.501ms.\n",
      "  dependency_optimizer: Graph size after: 186 nodes (0), 206 edges (0), time = 2.833ms.\n",
      "\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization/FusedBatchNormV3'\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization_1/FusedBatchNormV3'\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization_2/FusedBatchNormV3'\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization_3/FusedBatchNormV3'\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization_4/FusedBatchNormV3'\n",
      "WARNING:tensorflow:Didn't find expected Conv2D or DepthwiseConv2dNative input to 'batch_normalization_5/FusedBatchNormV3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-01 15:46:37.204424: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  remapper: Graph size after: 258 nodes (72), 308 edges (102), time = 11.329ms.\n",
      "  constant_folding: Graph size after: 186 nodes (-72), 212 edges (-96), time = 172.683ms.\n",
      "  arithmetic_optimizer: Graph size after: 186 nodes (0), 212 edges (0), time = 74.784ms.\n",
      "  dependency_optimizer: Graph size after: 180 nodes (-6), 200 edges (-12), time = 4.234ms.\n",
      "  remapper: Graph size after: 180 nodes (0), 200 edges (0), time = 1.859ms.\n",
      "  constant_folding: Graph size after: 180 nodes (0), 200 edges (0), time = 9.785ms.\n",
      "  arithmetic_optimizer: Graph size after: 180 nodes (0), 200 edges (0), time = 8.446ms.\n",
      "  dependency_optimizer: Graph size after: 180 nodes (0), 200 edges (0), time = 4.035ms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "    --input_format=tf_saved_model \\\n",
    "    --output_format=tfjs_graph_model \\\n",
    "    --signature_name=serving_default \\\n",
    "    --saved_model_tags=serve \\\n",
    "    ./keras_estimator_savedmodel/1622537169 \\\n",
    "    ./keras_estimator_savedmodel/json_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
