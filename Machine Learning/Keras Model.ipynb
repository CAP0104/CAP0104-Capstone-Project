{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from dataset import *\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "TRAIN_PATH = 'tf_train.csv'\n",
    "VALIDATION_PATH = 'tf_validation.csv'\n",
    "#TEST_PATH = 'test_processed.csv'\n",
    "MODEL_DIR = 'kmodel1'\n",
    "SUBMISSION_NAME = 'submission_keras.csv'\n",
    "TEST_PATH = 'test_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "STEPS = 100000\n",
    "BATCH_SIZE = 512\n",
    "DATASET_SIZE = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
    "               'dropoff_latitude', 'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [['nokey'], [1.0], ['2009-06-15 17:26:21 UTC'], [-74.0], [40.0], [-74.0], [40.7], [1.0], [2009], [6], [15],\n",
    "            [17], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['pickup_longitude_binned'] = pd.qcut(df['pickup_longitude'], 16, labels=False)\n",
    "    df['dropoff_longitude_binned'] = pd.qcut(df['dropoff_longitude'], 16, labels=False)\n",
    "    df['pickup_latitude_binned'] = pd.qcut(df['pickup_latitude'], 16, labels=False)\n",
    "    df['dropoff_latitude_binned'] = pd.qcut(df['dropoff_latitude'], 16, labels=False)\n",
    "\n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test(df):\n",
    "    #df = df.drop('pickup_datetime', axis=1)\n",
    "    \n",
    "    df['pickup_longitude_binned'] = pd.qcut(df['pickup_longitude'], 16, labels=False)\n",
    "    df['dropoff_longitude_binned'] = pd.qcut(df['dropoff_longitude'], 16, labels=False)\n",
    "    df['pickup_latitude_binned'] = pd.qcut(df['pickup_latitude'], 16, labels=False)\n",
    "    df['dropoff_latitude_binned'] = pd.qcut(df['dropoff_latitude'], 16, labels=False)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n",
    "    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relevant_distances(df):\n",
    "    # Add airpot distances and downtown\n",
    "    ny = (-74.0063889, 40.7141667)\n",
    "    jfk = (-73.7822222222, 40.6441666667)\n",
    "    ewr = (-74.175, 40.69)\n",
    "    lgr = (-73.87, 40.77)\n",
    "    df['downtown_pickup_distance'] = manhattan(ny[1], ny[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['downtown_dropoff_distance'] = manhattan(ny[1], ny[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['jfk_pickup_distance'] = manhattan(jfk[1], jfk[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['jfk_dropoff_distance'] = manhattan(jfk[1], jfk[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['ewr_pickup_distance'] = manhattan(ewr[1], ewr[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['ewr_dropoff_distance'] = manhattan(ewr[1], ewr[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['lgr_pickup_distance'] = manhattan(lgr[1], lgr[0], df['pickup_latitude'], df['pickup_longitude'])\n",
    "    df['lgr_dropoff_distance'] = manhattan(lgr[1], lgr[0], df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered(df):\n",
    "    lat1 = df['pickup_latitude']\n",
    "    lat2 = df['dropoff_latitude']\n",
    "    lon1 = df['pickup_longitude']\n",
    "    lon2 = df['dropoff_longitude']\n",
    "\n",
    "    latdiff = (lat1 - lat2)\n",
    "    londiff = (lon1 - lon2)\n",
    "    euclidean = (latdiff ** 2 + londiff ** 2) ** 0.5\n",
    "\n",
    "    # Add new features\n",
    "    df['latdiff'] = latdiff\n",
    "    df['londiff'] = londiff\n",
    "    df['euclidean'] = euclidean\n",
    "    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    # One-hot encoding columns\n",
    "    # Note, this is note the best way to one-hot encode features, but probably the simplest and will work here\n",
    "    df = pd.get_dummies(df, columns=['weekday'])\n",
    "    df = pd.get_dummies(df, columns=['month'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_function(features, labels=None, shuffle=False):\n",
    "    input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n",
    "        x={\"raw_input\": features},\n",
    "        y=labels,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset2(filename, mode, features_cols, label_col, default_value, batch_size=512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.io.decode_csv(records=value_column, record_defaults=default_value)\n",
    "            features = dict(zip(features_cols, columns))\n",
    "            label = features.pop(label_col)\n",
    "\n",
    "            features = tf.cast(features, dtype=tf.float32)\n",
    "            features = {\"raw_input\": add_engineered(features)}\n",
    "            label = tf.cast(label, dtype=tf.float32)\n",
    "            return features, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read lines from text files\n",
    "        # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "        textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "        dataset = textlines_dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        batch_features, batch_labels = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n",
    "\n",
    "        return batch_features, batch_labels\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load values in a more compact form\n",
    "data_train_types = {'key': 'str',\n",
    "             'fare_amount': 'float32',\n",
    "             'pickup_datetime': 'str',\n",
    "             'pickup_longitude': 'float32',\n",
    "             'pickup_latitude': 'float32',\n",
    "             'dropoff_longitude': 'float32',\n",
    "             'dropoff_latitude': 'float32',\n",
    "             'passenger_count': 'uint8',\n",
    "             'year': 'uint16',\n",
    "             'month': 'uint8',\n",
    "             'day': 'uint8',\n",
    "             'hour': 'uint8',\n",
    "             'weekday': 'uint8',\n",
    "             'night': 'uint8',\n",
    "             'late_night': 'uint8'}\n",
    "\n",
    "data_train_names = ['key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "              'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']\n",
    "\n",
    "data_test_types = {'key': 'str',\n",
    "             'pickup_datetime': 'str',\n",
    "             'pickup_longitude': 'float32',\n",
    "             'pickup_latitude': 'float32',\n",
    "             'dropoff_longitude': 'float32',\n",
    "             'dropoff_latitude': 'float32',\n",
    "             'passenger_count': 'uint8',\n",
    "             'year': 'uint16',\n",
    "             'month': 'uint8',\n",
    "             'day': 'uint8',\n",
    "             'hour': 'uint8',\n",
    "             'weekday': 'uint8',\n",
    "             'night': 'uint8',\n",
    "             'late_night': 'uint8'}\n",
    "\n",
    "data_test_names = ['key', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n",
    "              'passenger_count', 'year', 'month', 'day', 'hour', 'weekday', 'night', 'late_night']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE, dtype=data_train_types, usecols=[1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14], names=data_train_names)\n",
    "test = pd.read_csv(TEST_PATH, dtype=data_test_types, usecols=[0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13], names=data_test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "train = process(train)\n",
    "test = process(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "train = add_relevant_distances(train)\n",
    "test = add_relevant_distances(test)\n",
    "\n",
    "train = add_engineered(train)\n",
    "test = add_engineered(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted columns\n",
    "dropped_columns = ['pickup_longitude', 'pickup_latitude',\n",
    "                   'dropoff_longitude', 'dropoff_latitude']\n",
    "train_clean = train.drop(dropped_columns, axis=1)\n",
    "test_clean = test.drop(dropped_columns + ['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in train and validation (90% ~ 10%)\n",
    "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "train_labels = train_df['fare_amount'].values\n",
    "validation_labels = validation_df['fare_amount'].values\n",
    "train_df = train_df.drop(['fare_amount'], axis=1)\n",
    "validation_df = validation_df.drop(['fare_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train_df_scaled = scaler.fit_transform(train_df).astype(np.float32)\n",
    "validation_df_scaled = scaler.transform(validation_df).astype(np.float32)\n",
    "test_scaled = scaler.transform(test_clean).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(256, activation='relu', input_shape=(train_df_scaled.shape[1],), name='raw'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1, name='predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "raw (Dense)                  (None, 256)               10496     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 56,257\n",
      "Trainable params: 55,265\n",
      "Non-trainable params: 992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:63: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.estimator.RunConfig(model_dir=MODEL_DIR, save_summary_steps=5000, save_checkpoints_steps=5000)\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=input_function(train_df_scaled, train_labels, True),\n",
    "                                    max_steps=STEPS)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=input_function(validation_df_scaled, validation_labels, True),\n",
    "                                  steps=1000, throttle_secs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using the Keras model provided.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:434: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'kmodel1', '_tf_random_seed': None, '_save_summary_steps': 5000, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 5000 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:60: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:491: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='kmodel1\\\\keras\\\\keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: kmodel1\\keras\\keras_model.ckpt\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 22 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:906: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into kmodel1\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 257.50702, step = 0\n",
      "INFO:tensorflow:global_step/sec: 70.7118\n",
      "INFO:tensorflow:loss = 168.34755, step = 100 (1.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.161\n",
      "INFO:tensorflow:loss = 176.28938, step = 200 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.856\n",
      "INFO:tensorflow:loss = 205.54431, step = 300 (0.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.28\n",
      "INFO:tensorflow:loss = 171.738, step = 400 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.584\n",
      "INFO:tensorflow:loss = 141.31873, step = 500 (0.639 sec)\n",
      "INFO:tensorflow:global_step/sec: 171.768\n",
      "INFO:tensorflow:loss = 152.56058, step = 600 (0.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.98\n",
      "INFO:tensorflow:loss = 199.54095, step = 700 (0.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.625\n",
      "INFO:tensorflow:loss = 166.83376, step = 800 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.55\n",
      "INFO:tensorflow:loss = 141.54749, step = 900 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.381\n",
      "INFO:tensorflow:loss = 147.12866, step = 1000 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 175.924\n",
      "INFO:tensorflow:loss = 131.69235, step = 1100 (0.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 161.123\n",
      "INFO:tensorflow:loss = 185.21024, step = 1200 (0.621 sec)\n",
      "INFO:tensorflow:global_step/sec: 164.963\n",
      "INFO:tensorflow:loss = 131.7451, step = 1300 (0.606 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.812\n",
      "INFO:tensorflow:loss = 133.91594, step = 1400 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.013\n",
      "INFO:tensorflow:loss = 116.32184, step = 1500 (0.575 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.348\n",
      "INFO:tensorflow:loss = 176.34247, step = 1600 (0.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.964\n",
      "INFO:tensorflow:loss = 118.40637, step = 1700 (0.576 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.871\n",
      "INFO:tensorflow:loss = 96.666916, step = 1800 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 156.096\n",
      "INFO:tensorflow:loss = 122.28317, step = 1900 (0.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.992\n",
      "INFO:tensorflow:loss = 93.49359, step = 2000 (0.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.956\n",
      "INFO:tensorflow:loss = 112.79044, step = 2100 (0.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 172.179\n",
      "INFO:tensorflow:loss = 85.36517, step = 2200 (0.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 174.16\n",
      "INFO:tensorflow:loss = 70.81603, step = 2300 (0.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.833\n",
      "INFO:tensorflow:loss = 124.4264, step = 2400 (0.709 sec)\n",
      "INFO:tensorflow:global_step/sec: 154.942\n",
      "INFO:tensorflow:loss = 94.46944, step = 2500 (0.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 159.92\n",
      "INFO:tensorflow:loss = 107.27177, step = 2600 (0.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 169.588\n",
      "INFO:tensorflow:loss = 76.66082, step = 2700 (0.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 170.341\n",
      "INFO:tensorflow:loss = 63.935806, step = 2800 (0.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 168.163\n",
      "INFO:tensorflow:loss = 78.955215, step = 2900 (0.595 sec)\n",
      "INFO:tensorflow:global_step/sec: 160.989\n",
      "INFO:tensorflow:loss = 94.83783, step = 3000 (0.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 157.707\n",
      "INFO:tensorflow:loss = 80.1747, step = 3100 (0.633 sec)\n",
      "INFO:tensorflow:global_step/sec: 173.11\n",
      "INFO:tensorflow:loss = 83.68379, step = 3200 (0.578 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3280...\n",
      "INFO:tensorflow:Saving checkpoints for 3280 into kmodel1\\model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3280...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2021-05-31T15:41:22Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from kmodel1\\model.ckpt-3280\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Evaluation [200/1000]\n",
      "INFO:tensorflow:Evaluation [300/1000]\n",
      "INFO:tensorflow:Inference Time : 2.11043s\n",
      "INFO:tensorflow:Finished evaluation at 2021-05-31-15:41:24\n",
      "INFO:tensorflow:Saving dict for global step 3280: global_step = 3280, loss = 81.84913, mae = 7.8129654\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3280: kmodel1\\model.ckpt-3280\n",
      "INFO:tensorflow:Loss for final step: 148.38272.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': 81.84913, 'mae': 7.8129654, 'global_step': 3280}, [])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = keras.estimator.model_to_estimator(keras_model=model, config=run_config)\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from kmodel1\\model.ckpt-3280\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Output complete\n"
     ]
    }
   ],
   "source": [
    "SUBMISSION_NAME = 'sample_submission.csv'\n",
    "prediction = estimator.predict(input_function(test_scaled))\n",
    "\n",
    "prediction_df = pd.DataFrame(prediction)\n",
    "output_submission(test, prediction_df, 'key', 'fare_amount', SUBMISSION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
